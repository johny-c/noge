# data
dataset: maze
test_size: 0.2

# env
max_episode_steps: 500
reward_type: er_diff  # path_length | er_diff | sign_er_diff
node_history: 2
cat_features: Y  # any combination of {Y, S, N, B, D}
feature_range: [-0.5, 0.5]
meas_transform: minmax   # normal | lognormal | minmax | theory_minmax | identity
target_transform: normal  # normal | lognormal | identity | minmax

# training loop
init_eval: True
n_train_steps: 25_600  # 800_000  # 20000 with epsilon_end=0.45
n_test_episodes: 50
train_freq: 32         # env steps
test_freq: 512         # 7812   # grad steps
log_freq: 100          # grad steps
n_eval_artifacts: 0    # save this many episode artifacts at every evaluation (epoch)
save_model: True

# DFP / Q-learning
replay_capacity: 20_000
min_horizon: 4
epsilon_start: 1.0
epsilon_end: 0.15
exploration_frac: 1.0
input_meas_type: R
output_meas_type: R
gamma: 0.99
target_update_freq: 100

# optimization
batch_size: 32
lr: 0.0001
loss: mse  # mse | huber | ordinal

# other
device: cpu
seed: 1
data_seed: 1

# model
model:
    dim_hidden: 64   # 128
    nonlinearity: relu  # lrelu
    alpha: 0.2
    dropout: 0
    num_gnn_layers: 2
    num_meas_layers: 2
    num_joint_layers: 2
    use_goal: False
    output_activation: null